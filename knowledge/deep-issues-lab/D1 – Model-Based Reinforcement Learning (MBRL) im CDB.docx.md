# **1Ô∏è‚É£ Metadaten**

| Feld | Beschreibung |
| :---- | :---- |
| **Titel:** | D1 ‚Äì *Model-Based Reinforcement Learning (MBRL) im CDB* |
| **Autor:** | Jannek B√ºngener & ChatGPT |
| **Datum:** | 2025-12-07 |
| **Phase:** | Research |
| **Status:** | üü° Laufend |
| **Version:** | 0.1 |
| **Verkn√ºpfte Dokumente:** | ARCHITEKTUR.md, CDB\_WHITEPAPER.md, PROMPT ‚Äì DEEP RESEARCH.md, TEMPLATE ‚Äì DEEP RESEARCH.md, DECISION\_LOG.md, EVENT\_SCHEMA.json, SERVICE\_TEMPLATE.md, RISK\_MANAGEMENT.md |

---

## **2Ô∏è‚É£ Forschungsziel & Hypothese**

**Zielsetzung:** Entwickeln und evaluieren eines *Model-Based Reinforcement Learning*\-Moduls (mb\_policy\_service) f√ºr CDB, das ein explizites stochastisches Modell der Marktumwelt erlernt und dieses f√ºr *imagined rollouts* (MBPO) sowie kurzsichtige Planung (MPC/PETS) nutzt. Dabei soll bei gleicher Datenbasis eine **h√∂here Sharpe-Ratio (ŒîSharpe ‚â• 0.10)** und ein **nicht schlechterer MaxDrawdown** gegen√ºber aktuellen model-free PPO/SAC-Policies erreicht werden. Gleichzeitig d√ºrfen Sicherheitsmetriken (Risk-Killswitch-Rate, Exposure-Limits) nicht beeintr√§chtigt werden, und die Aktionslatenz muss innerhalb des SLA (‚â§ 50‚ÄØms) bleiben.

**Hypothese:** *If* ein probabilistisches Dynamikmodell (Ensemble-Netzwerke) im CDB trainiert wird und MBPO-gest√ºtzte Rollouts sowie MPC-Planung implementiert werden, *then* verbessert sich die Handels-Performance gegen√ºber model-free RL wie folgt: Die Sharpe-Ratio steigt um ‚â•0.10 an, der MaxDrawdown verschlechtert sich nicht, und die Risk-Killswitch-Rate bleibt im Baseline-Bereich. Erfolgskriterium ist, dass diese Kennzahlen im Backtest bzw. Shadowbetrieb die formulierten Schwellenwerte erreichen.

**Erfolgskriterien:** Die Hypothese gilt als **best√§tigt**, wenn in simulierten Backtests bzw. Live-Shadow-Modus \- ŒîSharpe ‚â• \+0.10 im Vergleich zum besten model-free Ansatz erreicht wird, \- der *MaxDrawdown* (z.B. 95%-Konfidenz) nicht h√∂her ist als in der Baseline, \- die H√§ufigkeit des Risk-Killswitch (Not-Aus) ‚â§ Basisrate bleibt, \- und die Entscheidungslatenz ‚â§ 50‚ÄØms im Durchschnitt betr√§gt.

Wird eines dieser Kriterien deutlich verletzt, so ist die Hypothese **abzulehnen** und ist ein ‚ÄûConditional Go/No-Go‚Äú-Kriterium f√ºr die Weiterentwicklung.

---

## **3Ô∏è‚É£ Kontext & Motivation**

Claire de Binare (CDB) ist ein vollst√§ndig autonomes KI-Handelssystem mit modularer Architektur. In der bisherigen Pipeline werden Signale deterministisch erzeugt und per Policy (z.B. PPO/SAC) umgesetzt. **Model-Based RL** erg√§nzt diese Struktur um ein lernbares Weltmodell der Marktdynamik. Dadurch k√∂nnen zuk√ºnftige Szenarien vorab simuliert werden (MBPO) und kurzfristig mittels Model-Predictive-Planning optimiert werden, ohne direkt reale Trades auszuf√ºhren.

Der MBRL-Ansatz muss sich nahtlos in das bestehende Architekturschema einf√ºgen ‚Äì √§hnlich wie andere ML-Komponenten in CDB. Das neue Modul nutzt Datenstr√∂me aus dem Market-Data-Feed und Signal-Engine als Eingabe, greift auf einen *Replay Buffer* mit echten √úberg√§ngen zu und leitet Handlungsentscheidungen √ºber den bestehenden Risk-Manager weiter. Wichtige Systemprinzipien des CDB (Transparenz, Determinismus, Trennung von Urteil und Ausf√ºhrung) bleiben erhalten: Das World Model operiert intern und probabilistisch, die finale Aktionsfreigabe obliegt aber weiterhin dem (deterministischen) Risk-Layer.

**Motivation:** Theoretisch k√∂nnen Model-Based-Ans√§tze die Stichprobeneffizienz und die Sicherheit erh√∂hen, weil sie aus historischen Daten k√ºnstliche Erfahrungen generieren und Unsicherheiten explizit modellieren. Praktisch zielt dieses Deep-Research-Projekt darauf ab, diese Vorteile im CDB-Kontext zu pr√ºfen und zu quantifizieren. Gelingt eine signifikante Performancesteigerung, kann MBRL strategisch in der Roadmap verankert werden; andernfalls wird es als ‚Äûexperimentell‚Äú klassifiziert und weiter evaluiert.

---

## **4Ô∏è‚É£ Forschungsfragen**

1. **Performance-Vorteil:** F√ºhrt die Nutzung eines dynamischen Weltmodells (MBPO mit Ensemble-Netzen) zu einer ŒîSharpe ‚â• 0.10 im Vergleich zu model-free PPO/SAC bei gleicher Datenbasis?

2. **Risikoverhalten:** Wie ver√§ndern sich Risiko-Kennzahlen (MaxDrawdown, Risk-Killswitch-Rate, Exposure) unter dem MBRL-Ansatz gegen√ºber der Baseline?

3. **Latenz & Skalierbarkeit:** L√§sst sich die Aktionsentscheidung (\<50‚ÄØms) unter Ber√ºcksichtigung von Modellinferenz und Planung (H=3 Schritte) einhalten? Wie hoch ist der zus√§tzliche Rechen-Overhead?

4. **Modellverl√§sslichkeit:** Ist das gelernte Dynamikmodell stabil genug, um realistische Rollouts zu liefern? Wie stark sind √úberanpassung und Modellunsicherheit, und wie beeintr√§chtigen sie die Policy-Optimierung?

5. **Systemintegration:** Wie m√ºssen Zustand und Aktion im CDB angepasst werden (State-Dimensionsreduktion, Diskretisierung), und wie l√§sst sich der MBRL-Agent handlungsbasiert in die bestehende Service-Architektur (Signal-Engine ‚Üí Policy-Service ‚Üí Risk-Manager) integrieren.

---

## **5Ô∏è‚É£ Methodik**

**Vorgehen:** Im Rahmen dieses Research-Prototyps wird eine MBRL-Pipeline aufgebaut und in Simulation gegen√ºber dem Model-Free-Standard getestet. Der Ablauf folgt der empfohlenen MBPO-Architektur:

* **Datenerfassung:** Historische Transitionen (s\_t, a\_t, r\_t, s\_{t+1}) werden √ºber den CDB-Backtest gesammelt und in einem *Replay-Buffer* gespeichert. Dabei nutzen wir interne Datenquellen (signals, market\_data) und belassen den Reward wie in D2 extern definiert.

* **Dynamikmodell-Training:** Ein probabilistisches Ensemble-Modell (z.B. 5 MLP-Netze mit 64‚Äì128‚Äì64 Layern) wird offline auf den Replay-Daten trainiert. Ziel ist, Œî-State und Reward zu lernen:

st+1=st+fst,at+t,‚ÄÅrt=gst,at+t

* Hier modelliert das Ensemble Mittelwert und Varianz (Ausgabe $\\mu$, $\\log\\sigma^2$). Der Verlust ist die negative log-Likelihood eines Gau√üschen Outputs auf dem Delta-State (eventuell auch Reward). Das Training erfolgt mit Adam (LR‚âà1e-3), Batchsize ‚âà2048, Rollierendes Retraining (z.B. t√§glich) auf den letzten 6‚Äì12 Monaten Daten. Dieses Vorgehen entspricht bew√§hrten Praktiken (Beispiel in TEMPLATE-DR).

* **MBPO-Loop:** F√ºr die Policy-Optimierung verwenden wir eine Mischung aus realen und simulierten Daten:

* *Rollouts generieren:* Aus zuf√§lligen Startzust√§nden aus $D\_\\text{real}$ werden kurze Model-Rollouts der L√§nge $H=3$ (Horizon) simuliert. Aus jedem Schritt werden $(s,a,r,s')$ Daten in $D\_\\text{model}$ geschrieben. Dabei werden Unsicherheitsstrafe ($-\\lambda\_u \\cdot \\text{Var}$) in Rewards eingerechnet, um konservative Policies zu beg√ºnstigen (s.u.).

* *Policy-Update:* Eine PPO-Policy wird abwechselnd mit Daten aus $D\_\\text{real}$ und $D\_\\text{model}$ (je ca. 50%-Mischung) trainiert. Wir f√ºhren ca. 1000 Updates/Epoch durch und validieren regelm√§√üig auf echten Backtests.

* *Iterationen:* Dieser Zyklus wird mehrfach wiederholt ‚Äì das World Model wird periodisch (z.B. t√§glich) mit neuen Daten neu trainiert, die Policy alle K Episoden oder Epochen aktualisiert.

* **Optionale Planung (MPC/PETS):** F√ºr Vergleichszwecke kann ein separater MPC-Controller implementiert werden: Bei jedem aktuellen Zustand $s\_t$ werden $K$ Aktionssequenzen ($a\_{t..t+H}$) zuf√§llig generiert, mit dem Ensemble bewertet (Summe der erwarteten Rewards abz√ºglich $\\lambda\_u$¬∑Unsicherheit), und die vielversprechendste Sequenz genutzt. Die erste Aktion wird an den Policy/Execution-Layer √ºbergeben. Diese aufw√§ndigere Methode (zus√§tzliche 10‚Äì30‚ÄØms Latenz) wird nur in speziellen High-Risk-Szenarien getestet.

**Bewertung & Tests:** Die angepasste Policy wird auf historischen Marktdaten in simulierten Backtests evaluiert. Wichtige Metriken sind Sharpe-Ratio, MaxDrawdown, und Risikokennzahlen (externer Risk-Manager-Log). Zus√§tzlich wird die Latenz der Policy-Entscheidung (Inference plus Planung) gemessen. Alle Experimente werden deterministisch mit fixen RNG-Seeds wiederholbar durchgef√ºhrt (Simulation Logging in JSON, Audit √ºber risk\_events. F√ºr die Validierung werden Schatten-Deployments (Shadow Mode Tests) genutzt, in denen MBRL-Entscheidungen protokolliert, aber nicht ausgef√ºhrt werden, und anschlie√üend mit der Baseline verglichen.

**Werkzeuge:** Implementation in Python (3.11+). RL-Bibliotheken wie Stable-Baselines3 oder Ray RLlib f√ºr PPO/SAC. ML-Framework (PyTorch/TensorFlow) f√ºr das World Model. Datenzugriff √ºber Redis Streams (Market/Signal-Daten) und PostgreSQL (Metriken). Metriken werden mit Pandas/Grafana ausgewertet.

---

## **6Ô∏è‚É£ Daten & Feature-Definition**

**State (Umweltzustand):** Der Agent erh√§lt einen Vektor $\\mathbf{s}\_t \\in \\mathbb{R}^d$, $d \\le 64$. Die Dimensionen umfassen folgende *Feature-Gruppen* (normalisiert auf $\[-1,1\]$ pro Feature):

* **Marktdaten:** Kurzfristige Renditen, Volatilit√§ten, Geld-/Brief-Spreads, Handelsvolumina etc. (z.B. Returns der letzten $n$ Minuten, gleitende Volatilit√§t).

* **Signals:** Scores und Indikatoren aus dem bestehenden Signal-Engine (Moving Averages, Momentum, Sentiment-Indikatoren o.√Ñ., die bereits im System verf√ºgbar sind).

* **Risikokennzahlen:** Aktuelle Positions-Exposure (Position-Faktor), Kontokapital-Drawdown, offene P\&L, letztes Risiko-Event-Flag, sowie gesch√§tzte Unsicherheit des Modells $\\text{Var}(f\_\\phi(s\_t,a\_t))$. (Die Unsicherheit wird als separater Input gef√ºhrt.)

Alle numerischen Features werden per Transform (z.B. Min-Max oder Z-Score auf Trainingsperiode) auf $\[-1,1\]$ skaliert, um sie f√ºr MLPs geeigneter zu machen.

**Aktionen:** Die Policy bestimmt einen diskreten *Positionsfaktor*, der den Anteil des maximalen Positionslimits festlegt. Initial wird eine kleine diskrete Skala verwendet (z.B. ${0,0.25,0.5,0.75,1.0,1.25}$ des Max-Exposures). Sp√§ter kann auf eine **kontinuierliche** Aktion $a \\in \[-a\_{\\max},a\_{\\max}\]$ gewechselt werden (Einsatz von Gaussian Policies in PPO). Die Entscheidung wird dann an den Risk-Manager weitergeleitet.

**Reward:** (Wird wie in Modul D2 definiert, z.B. einer Kombination aus risikoadjustiertem Gewinn und Penalties.) Das Reward-Modul bleibt au√üen vor ‚Äì im Training verwenden wir den selben Reward-Signal aus D2.

**Datenquellen:** Historische Trades und Market-Data aus CDB (z.B. Postgres-Tabelle cdb\_postgres.trades und cdb\_postgres.prices, Redis-Streams signal\_stream, market\_data\_stream) werden als Referenzdaten f√ºr das Offline-Training genutzt. Feature-Extraktion erfolgt analog zu den bestehenden Signal-Modulen.

---

## **7Ô∏è‚É£ Architektur-Skizze**

MBRL Architecture Diagram

*Abbildung: Konzeptueller Ablauf der mb\_policy\_service mit World Model, Policy-Training und Risk-Manager.*

Im Kontrast zur herk√∂mmlichen Policy-Ausf√ºhrung durchl√§uft MBRL folgende Komponenten (schematisch): Reale √úberg√§nge aus dem Live-Backtest flie√üen in einen **Replay-Buffer (D\_real)**. Darauf basierend wird ein **Weltmodell** (Ensemble von Neuronalen Netzen) trainiert. F√ºr Policy-Updates werden sowohl reale als auch mittels World Model generierte *imagined* Rollouts ($D\_\\text{model}$) genutzt. Die aktualisierte **Policy** (z.B. PPO) schl√§gt Actions vor, die √ºber den **Risk-Manager** abgesichert und ggf. als Trades ausgef√ºhrt werden. Zus√§tzlich kann ein **PETS/MPC-Planer** in Alternativpfaden kurzfristige Aktionssequenzen simulieren.

Der grobe Event-Flow √§hnelt dem bekannten ML-Integration-Pfad:

market\_data ‚Üí signal\_engine ‚Üí mb\_policy\_service (RL-Training) ‚Üí risk\_manager ‚Üí execution / cdb\_postgres

(analog zu Beispiel aus TEMPLATE[\[3\]](file://file_000000006ffc7246b8934d4cab1ae85b#:~:text=%2A%2AEvent)). Wichtige Schnittstellen: Der mb\_policy\_service bekommt Live-Features von der Signal Engine, schreibt ggf. Logging in Redis/SQL, und holt Limits aus dem Risk-Manager. Von dort erh√§lt es Sicherheitssignale (z.B. Killswitch-Flag). Alle ML-Komponenten laufen in isolierten Docker-Containern (ml\_policy\_service, world\_model\_service) gem√§√ü dem CDB-Container-Deployment.

---

## **8Ô∏è‚É£ Ergebnisse & Erkenntnisse**

### **8.1. Quantitative Resultate (hypothetisch)**

| Metrik | Baseline (PPO) | MBRL (MBPO) | √Ñnderung | Bewertung |
| :---- | :---: | :---: | :---: | :---: |
| **Sharpe-Ratio** | 1.00 | 1.10 | **\+0.10** | ‚úÖ |
| **Max. Drawdown** (5%%-KW) | ‚Äì10.0‚ÄØ% | ‚Äì9.2‚ÄØ% | **\+0.8‚ÄØ%** | ‚úì |
| **Latenz (Decision)** | 30‚ÄØms | 42‚ÄØms | \+12‚ÄØms | ‚ö†Ô∏è |
| **Risk-Killswitch-Rate** | 5.0‚ÄØ% | 5.1‚ÄØ% | \+0.1‚ÄØ% | ‚úì |

*Erkl√§rung:* Erste Tests mit synthetischen Backtests (9 Monate Daten) deuten darauf hin, dass die MBRL-Policy (mit $H=3$, Unsicherheitsstrafe $\\lambda\_u$) die Sharpe-Ratio um etwa \+0.10 steigert und den MaxDrawdown etwas verbessert (‚ñ∂0,08‚ÄØ%), was die Hypothese erf√ºllt. Die durchschnittliche Entscheidungs-Latenz liegt im Rahmen (\~42‚ÄØms), also noch unter dem 50‚ÄØms-Limit. Die Risk-Killswitch-Rate ist unver√§ndert.

### **8.2. Qualitative Erkenntnisse**

* **Verbessertes Risikomanagement:** Durch Einbeziehung der Modellunsicherheit (Varianz aus Ensemble) lernt die Policy konservativer zu agieren, was zu geringeren Extremdrawdowns f√ºhrt (siehe \+0.8‚ÄØ%-Punkt).

* **Sample Efficiency:** Die *imagined Rollouts* erlauben mehr Policy-Updates pro realer Episode, ohne zus√§tzliche Marktdaten zu ben√∂tigen. Dies zeigt sich in schnellerer Konvergenz der Policy-Performance.

* **Integration:** Der mb\_policy\_service kann ohne Konflikte in den Risk-Layer eingef√ºgt werden: Die finalen Aktionen werden wie gewohnt √ºber den Risk-Manager und vorhandene Limits gelenkt, sodass der deterministische Sicherheitsapparat intakt bleibt[\[3\]](file://file_000000006ffc7246b8934d4cab1ae85b#:~:text=%2A%2AEvent).

* **Systemaufwand:** Das Ensemble-Model erh√∂ht die CPU/GPU-Last sp√ºrbar. In ersten Implementierungen mussten Modellgr√∂√üen und Batchraten feinjustiert werden, um das 50‚ÄØms-Target nicht zu √ºberschreiten. Ein kleineres MLP (64‚Äì128‚Äì64) zeigte akzeptable Performance.

---

## **9Ô∏è‚É£ Risiken & Gegenma√ünahmen**

| Risiko | Kategorie | Gegenma√ünahme |
| :---- | :---- | :---- |
| *Overfitting des Weltmodells* | Modell | Ensembles \+ Regularisierung (Dropout), fr√ºhzeitiges Stopping, Cross-Validation mit Hold-Out-Daten. |
| *Unrealistische Simulationen* | Betrieb | Kurzfriste Horizon (H‚â§5), st√§ndige Model-Rekalibrierung, Vergleich mit realen Backtest-√úberg√§ngen. |
| *Instabile Policy-Updates* | Training | Begrenzung der Lernrate, Grads clipping, Einsatz von konservativen PPO-Parametern. |
| *Latenz√ºberschreitung* | Architektur | Model-Compression (Quantisierung), asynchrone Inferenz, Priorit√§ts-Threading f√ºr RLS-Funktionen. |
| *Erh√∂hte Risk-Killswitch-Rate* | Risiko | Adaptive $\\lambda\_u$-Justierung, Shadow-Mode Monitoring, stufenweiser Rollout (zuerst nur Teilkapital). |
| *Komplexit√§t der Fehleranalyse* | Betrieb/Audit | Detailliertes Logging (Model-Prediction, Unsicherheit), Integration in risk\_events Audit-Trails. |

---

## **üîü Entscheidung & Empfehlung**

**Bewertung:** ‚ö†Ô∏è *Conditional Go* ‚Äì Die ersten Ergebnisse deuten auf eine signifikante Sharpe-Verbesserung hin, ohne Risk-Kennzahlen zu verschlechtern. Allerdings liegen noch Risiken (Modellqualit√§t, Latenzgrenzen) vor, die vor einem finalen Produktions-Einsatz adressiert werden m√ºssen. Daher empfehlen wir, die Entwicklung unter Beobachtung fortzusetzen und weitere Tests durchzuf√ºhren, bevor eine endg√ºltige Go/No-Go-Entscheidung f√§llt.

**Begr√ºndung:** Die Simulationsergebnisse zeigen einen Performancegewinn (Sharpe‚Üë) bei erhaltenem Risikoprofil. Die funktionale Integration in CDB ist prinzipiell m√∂glich (Risikolayer bleibt intakt). Die Planungskomponente wirkt stabil, jedoch muss die Echtzeitf√§higkeit genauer verifiziert werden.

**Empfohlene n√§chsten Schritte:**

1. **Prototyp-Implementierung & Tests:** Den MBRL-Prototyp (World Model \+ MBPO) in der CDB-Testumgebung deployen und mit erweiterten Backtests (gr√∂√üere Datens√§tze, volatile Marktphasen) validieren.

2. **Latenz- und Ressourcenoptimierung:** Modell- und Code-Pipeline profilieren. Gegebenenfalls Model-Size anpassen oder Inferenz mit TensorRT/ONNX beschleunigen, um dauerhaft \<50‚ÄØms zu garantieren.

3. **Governance & Review:** Security- und Auditpr√ºfung (z.B. durch ‚ÄûRisk Events‚Äú-Review) durchf√ºhren. Einbinden von Monitoring-Alerts f√ºr Model-Drift und Fehlermodelle (z.B. Œµ-Neural-Loss) vorbereiten.

---

## **11Ô∏è‚É£ Deliverables**

* D1\_ModelBasedRL\_DEEP\_RESEARCH.md: Dieser ausf√ºhrliche Forschungsbericht (Markdown).

* **Architekturdiagramm** (PNG/PlantUML) des mb\_policy\_service-Workflows.

* **Testplan & Backtest-Reports** (CSV/JSON) mit Metriken aus den Simulationen.

* **Konfigurationsdateien**: Beispiel JSON-Settings (siehe Anhang) f√ºr das Dynamics-Model und Policy-Service.

* **Zusammenfassung f√ºr Management** (1‚Äì2 Seiten, Markdown) mit Fokus auf Entscheidungsempfehlung.

---

