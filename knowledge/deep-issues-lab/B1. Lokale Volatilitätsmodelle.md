---
id: CDB-DR-B1
title: 'Analyse lokaler Volatilitätsmodelle'
subtitle: 'Anwendung von Dupire- und HV-Smoothing-Methoden im Risikomanagement'
author: 'Jannek Buengener, ChatGPT, Claude Code, und Gemini'
date: '2025-12-17'
status: 'Refactored'
tags:
  - Volatilität
  - Lokale Volatilität
  - Dupire
  - EWMA
  - Risikomanagement
---

# Analyse lokaler Volatilitätsmodelle

> **Management Summary**
>
> Dieses Dokument analysiert lokale Volatilitätsmodelle und deren Anwendung im algorithmischen Handel. Im Gegensatz zu Modellen mit konstanter oder stochastischer Volatilität behandeln lokale Volatilitätsmodelle die Volatilität als eine deterministische Funktion des aktuellen Preises und der Zeit: `σ = σ(S, t)`.
>
> Der Fokus liegt auf zwei zentralen Ansätzen:
> 1.  **Theoretische Modelle:** Die Herleitung der lokalen Volatilitätsoberfläche aus Optionspreisen mittels der **Dupire-Gleichung**.
> 2.  **Praktische Modelle:** Die Schätzung der **historischen Volatilität (HV)** aus Preiszeitreihen und deren Glättung mittels **EWMA (Exponentially Weighted Moving Average)**.
>
> Ziel ist es, ein robustes und performantes Konzept für die Echtzeit-Berechnung und Integration von Volatilitätsmaßen in die Risikomanagement- und Handelsprozesse von *Claire de Binare* zu definieren.

--- 

## 1. Grundlagen der lokalen Volatilität

### 1.1. Abgrenzung zu anderen Volatilitätsmodellen

-   **Lokale Volatilität:** Die Volatilität `σ(S, t)` ist eine deterministische Funktion von Preis `S` und Zeit `t`. Sie kann so kalibriert werden, dass sie die Preise aller europäischen Vanilla-Optionen exakt abbildet.[^1, ^2]
-   **Stochastische Volatilität (z.B. Heston-Modell):** Die Volatilität folgt einem eigenen, zusätzlichen Zufallsprozess. Dies führt zu realistischeren Preis-Pfaden, aber einer ungenaueren Kalibrierung auf die anfängliche Volatilitätsoberfläche.[^3, ^4]
-   **Kombinierte Modelle (SLV - Stochastic Local Vol):** Versuchen, die Vorteile beider Ansätze zu vereinen, sind aber komplexer.[^5]

Für die dynamische Risikosteuerung ist eine schnelle Anpassung an die *aktuelle* Volatilität entscheidend, weshalb der Fokus auf lokalen bzw. historischen Maßen liegt.

### 1.2. Das Dupire-Modell

Bruno Dupire (1994) zeigte, wie man aus der Oberfläche gehandelter Optionspreise die lokale Volatilitätsfunktion `σ_lok(S, t)` ableiten kann.[^6] Die Dupire-Gleichung verknüpft die partiellen Ableitungen des Optionspreises `C(K, T)` nach Laufzeit `T` und Strike `K`:[^7]

```latex
\sigma_{\text{lok}}^2(K,T) \;=\; \frac{\partial C(K,T)/\partial T \;+\; r\,K\,\partial C/\partial K}{\tfrac{1}{2}\,K^2\,\partial^2 C/\partial K^2}
```

Diese Gleichung ist fundamental, da sie es ermöglicht, aus Markt-Implied-Volatilitäten ein konsistentes dynamisches Modell für den Basiswert abzuleiten.[^8, ^9] Für die Anwendung im reinen Futures-Handel ist dieser Ansatz jedoch weniger relevant, wenn keine liquiden Optionsmarktdaten zur Verfügung stehen.

## 2. Historische Volatilität (HV) und Glättungsmethoden

In der Praxis wird daher meist die **historische Volatilität (HV)** verwendet. Sie wird aus vergangenen Preisdaten berechnet und spiegelt die *realisierte* Schwankungsbreite wider. Um das Rauschen in den Rohdaten zu reduzieren, werden Glättungsmethoden eingesetzt.

### 2.1. Rolling Window (Gleitendes Fenster)

-   **Funktionsweise:** Berechnung der Standardabweichung der Renditen über die letzten `N` Perioden.
-   **Nachteil:** Reagiert abrupt, wenn alte, volatile Datenpunkte aus dem Fenster fallen. Alle Punkte im Fenster werden gleich gewichtet.

### 2.2. EWMA (Exponentially Weighted Moving Average)

-   **Funktionsweise:** Neuere Datenpunkte werden exponentiell höher gewichtet als ältere. Dies wird durch einen Glättungsfaktor `λ` (oft 0.94 für Tagesdaten) gesteuert.[^14]
-   **Formel:** `σ²_t = λ · σ²_{t-1} + (1-λ) · r²_{t-1}`[^15]
-   **Vorteile:**
    -   Passt sich adaptiv an neue Informationen an.[^16, ^17]
    -   "Vergisst" alte Daten allmählich, was zu glatteren Schätzungen führt.
    -   Sehr performant und in `O(1)` pro neuem Datenpunkt aktualisierbar.

**EWMA ist aufgrund seiner Einfachheit, Effizienz und der etablierten Nutzung (z.B. im J.P. Morgan RiskMetrics-Standard) die bevorzugte Methode für die Echtzeit-HV-Schätzung im algorithmischen Handel.**

### 2.3. Python-Implementierung (Beispiel)

Mittels `pandas` lässt sich die Berechnung von Rolling- und EWMA-Volatilität effizient implementieren:

```python
import numpy as np
import pandas as pd

# Annahme: 'prices' ist eine pandas Series mit Preisdaten
prices = pd.Series(...)
returns = prices.pct_change().dropna()

# 1. Rolling-Window Volatilität (20 Perioden)
window = 20
roll_vol = returns.rolling(window).std(ddof=0) * np.sqrt(252) # Annualisiert

# 2. EWMA-Volatilität (Lambda = 0.94)
lambda_ = 0.94
ewm_var = returns.pow(2).ewm(alpha=(1 - lambda_), adjust=False).mean() # EWMA Varianz
ewm_vol = np.sqrt(ewm_var) * np.sqrt(252) # Annualisiert

print(f"Aktuelle 20T-HV: {roll_vol.iloc[-1]:.4f}")
print(f"Aktuelle EWMA-HV: {ewm_vol.iloc[-1]:.4f}")
```

## 3. Architektur und Integration in CDB

### 3.1. Datenquellen und Speicherung

-   **Datenquelle:** Die HV-Berechnung basiert auf den Preiszeitreihen, die das System ohnehin verarbeitet. Ein dedizierter Service kann diese Daten nutzen.
-   **Speicherung in Redis:** Für den schnellen Zugriff im Millisekundenbereich werden die berechneten Volatilitätswerte in Redis gespeichert. Ein Hash pro Symbol (z.B. `vola_curve:BTCUSDT`) ist hierfür ideal. Er kann Werte für verschiedene Zeithorizonte enthalten:
    -   `volatility_5m`: Kurzfristige Vola
    -   `volatility_1h`: Mittelfristige Vola
    -   `volatility_24h`: Längerfristige Vola

### 3.2. Implementierung als Microservice

Aus Gründen der Entkopplung und Wiederverwendbarkeit wird empfohlen, die Volatilitätsberechnung in einem eigenen Microservice (`cdb_risk_vola_model`) zu kapseln.
-   **Aufgabe:** Dieser Service abonniert Marktdaten, berechnet kontinuierlich die HV (mittels EWMA) für verschiedene Zeithorizonte und schreibt die Ergebnisse in die entsprechenden Redis-Hashes.
-   **Vorteil:** Andere Services (Risk Manager, Signal Engine) können diese "fertigen" Volatilitätsdaten einfach aus Redis abrufen, ohne die Berechnung selbst durchführen zu müssen. Die rechenintensive Logik ist vom kritischen Order-Pfad entkoppelt.

## 4. Anwendung im Risk Manager und der Signal Engine

Die berechnete Volatilität dient als zentraler Input zur dynamischen Anpassung der Handelsstrategie:

1.  **Dynamische Positionsgrößen (Volatility Targeting):** Die Positionsgröße wird invers zur Volatilität skaliert, um das Portfoliorisiko konstant zu halten. Eine höhere Volatilität führt automatisch zu kleineren Positionen.[^33, ^34]
    `Position Notional = (Equity * TargetVol) / AssetVol`

2.  **Anpassung des Marktregimes:** Die Volatilitätshöhe dient als Indikator für das Marktregime (`Normal`, `Stressed`, `Panic`), was zu vordefinierten Aktionen führt (z.B. Halbierung der Positionsgröße im `Stressed`-Regime, Handelsstopp im `Panic`-Regime).[^27, ^28]

3.  **Dynamische Stop-Loss-Setzung:** Stop-Loss-Abstände können an die aktuelle Volatilität angepasst werden (z.B. via ATR), um in volatilen Phasen nicht vorzeitig ausgestoppt zu werden.

4.  **Regulierung der Handelsfrequenz:** In Phasen extrem hoher Volatilität kann ein "Cooldown"-Mechanismus aktiviert werden, der die Handelsfrequenz reduziert, um das Risiko von Fehlsignalen durch Marktrauschen zu senken.[^37]

**Fazit:** Lokale bzw. historische Volatilitätsmodelle sind ein entscheidender Baustein für ein robustes, adaptives Risikomanagement. Die Implementierung einer geglätteten HV-Schätzung (EWMA) in einem dedizierten Microservice und deren Integration in die bestehende Risk-Engine von *Claire de Binare* ist technisch unkompliziert und strategisch von hohem Wert.

--- 

