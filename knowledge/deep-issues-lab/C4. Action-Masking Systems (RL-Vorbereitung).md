---
id: CDB-DR-C4
title: 'Action Masking in Reinforcement Learning'
subtitle: 'Determinismus und Sicherheit für KI-Agenten im Finanzhandel'
author: 'Jannek Buengener, ChatGPT, Claude Code, und Gemini'
date: '2025-12-17'
status: 'Refactored'
tags:
  - Action Masking
  - Safe RL
  - Reinforcement Learning
  - Finanzhandel
  - Sicherheit
---

# Action Masking in Reinforcement Learning

> **Management Summary**
>
> Dieses Dokument spezifiziert die Integration von **Action Masking (AM)** als deterministischen Sicherheitsmechanismus in die Reinforcement Learning (RL)-Policy-Engine von *Claire de Binare*. Im algorithmischen Finanzhandel ist die Einhaltung regulatorischer und proprietärer Risikogrenzen nicht nur durch das Reward-Signal anzustreben, sondern **zwingend** durch die Systemarchitektur zu gewährleisten.
>
> Action Masking (AM) bietet hierfür eine robuste Lösung, indem es den Aktionsraum des RL-Agenten dynamisch auf zulässige Aktionen beschränkt. Dies erhöht nicht nur die Sicherheit und die Trainingseffizienz, sondern stellt auch sicher, dass der Agent niemals gegen harte finanzielle Constraints verstößt. Das Dokument behandelt die mathematischen Grundlagen, Implementierungsdetails in gängigen RL-Frameworks und die architektonische Umsetzung in einer Microservice-Umgebung mit Redis Cluster.

---

## 1. Theoretische Fundierung des Safe Reinforcement Learning (Safe RL) und Action Masking

### 1.1. Action Masking (AM) im Kontext von Constrained MDPs (C-MDPs)

Finanz-Trading-Systeme operieren in Umgebungen, in denen Fehler im Entscheidungsprozess katastrophale Folgen haben können. Finanzielle Limits (z.B. maximale Positionsgröße, Circuit Breaker) sind **harte, nicht verhandelbare Sicherheitsanforderungen**.[^1]

-   **Hard Masking (Shielding):** AM implementiert eine externe Sicherheitsschicht, die garantiert, dass nur zulässige Aktionen in die Umgebung *überführt* werden.[^2, ^4, ^6] Dies ist die einzig zulässige Methode für die finanzielle Policy-Engine von Claire.
-   **Vorteile für Trainingseffizienz:** AM reduziert den effektiven Aktionsraum (`A`) dynamisch auf die Menge der gültigen Aktionen (`A_valid ⊆ A`), was verhindert, dass der Agent Zeit mit der Exploration ungültiger Aktionen verschwendet. Dies erhöht die Sample Efficiency und beschleunigt die Konvergenz des Policy-Gradienten.[^7, ^8]

### 1.2. Mathematische Formulierung und Logit-Manipulation

Für diskrete Aktionsräume wird Action Masking durch eine Modifikation der Policy-Logits implementiert, bevor die finale Wahrscheinlichkeitsverteilung über die Softmax-Funktion generiert wird.

-   **Action Mask `M(s, a)`:** Ein binärer Vektor, der die Zulässigkeit jeder diskreten Aktion `a` im Zustand `s` anzeigt:

    ```
    M(s,a) = { 1 wenn Aktion a zulässig ist
             { 0 wenn Aktion a unzulässig ist
    ```

-   **Logit-Modifikation:** Ungültige Aktionen (`a` unzulässig) erhalten einen extrem kleinen Logit-Wert (negative Unendlichkeit `→-∞`), wodurch ihre Wahrscheinlichkeit nach Softmax effektiv 0 wird.[^7, ^9]

    ```latex
    L^{\text{masked}}(s,a) = L(s,a) + \log(M(s,a))
    ```
    ```latex
    P(a|s) = \frac{\exp(L^{\text{masked}}(s,a))}{\sum_{a' \in A} \exp(L^{\text{masked}}(s,a'))}
    ```

### 1.3. Implikationen der Hard Constraint Enforcement

-   **Folgen der Null-Gradienten:** Der Agent erhält für maskierte Aktionen keine Gradienten, was die Sample Efficiency verbessert, aber auch bedeutet, dass die Policy nicht aktiv lernt, warum diese Aktionen ungültig sind.[^10]
-   **Risiko des Policy-Bias:** Sollte das externe Action-Masking-System (Constraint Logic Engine) ausfallen, würde die Policy hochgradig unvorhersehbare und potenziell katastrophale Aktionen wählen. Dies unterstreicht die Notwendigkeit höchster Robustheit des Masking-Systems.
-   **Safe Default Action:** Eine dedizierte "Hold" (No-Trade) Aktion (`Act_Hold`) muss definiert werden, deren Maskenwert **immer** auf 1 gesetzt ist. Dies gewährleistet eine sichere Standardreaktion in kritischen Zuständen.[^11]

## 2. Implementierungsdetails in RL-Frameworks

Die Integration von Action Masking muss die architektonischen Besonderheiten der gewählten RL-Frameworks berücksichtigen.

### 2.1. `stable-baselines3 (SB3)` und `sb3-contrib`

-   **Abhängigkeit:** SB3 unterstützt Action Masking nicht nativ; die Funktionalität wird über `sb3-contrib` (z.B. `MaskablePPO`) bereitgestellt.[^12]
-   **Integration:** Die Maske muss direkt von der Custom Environment über die Methode `action_masks()` bereitgestellt werden.
-   **Einschränkungen:** Bei vektorisierten Umgebungen (`SubprocVecEnv`) wird die I/O-Latenz für den Masken-Lookup direkt in den synaptischen Pfad der Environment eingebettet, was die Latenz des Samplings dominiert.

### 2.2. `Ray[rllib]` und das Custom RLModule (Mask-as-Observation Pattern)

-   **Architektur:** Bietet eine überlegenere architektonische Lösung, die das **Mask-as-Observation**-Muster nutzt, welches die Masken-Generierung vom internen Logit-Masking der Policy entkoppelt.[^13, ^14]
-   **Observationsraum:** Die Environment verwendet einen `gymnasium.spaces.Dict` Observation Space, der den eigentlichen Observationsvektor (`
