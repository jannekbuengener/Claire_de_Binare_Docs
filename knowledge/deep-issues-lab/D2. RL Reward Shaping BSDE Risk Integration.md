Deep Research-Bericht: D2. RL Reward Shaping BSDE Risk Integration.docx  
Quellenübersicht  
Deep Research – RL Reward Shaping / BSDE Risk Integration (Final Version)  
1\. Executive Summary  
Ein RL-Agent lernt durch die Interaktion von **State → Action → Reward** (siehe Abbildung). Im Finanzbereich werden dabei häufig risikoadjustierte Kennzahlen wie der **Sharpe-Quotient** als Reward herangezogen\[1\]. Um zugleich Rendite und Risiko abzuwägen, wird in der Literatur ein **multidimensionales Reward-Design** vorgeschlagen, das Ertrag und Risiko in Einklang bringt\[2\]\[3\]. In diesem Projekt wird ein mehrstufiger Ansatz verfolgt: Zunächst ein Reward aus normalisiertem Profit und Drawdown (N1), anschließend optional eine BSDE-basierte Risikoschätzung als Strafterm (N2). Solche kombiniert-risikosensitiven Belohnungen gelten als „monoton“ (Mehr Profit ⟹ höheres Reward, mehr Risiko ⟹ niedrigeres Reward) und sind differenzierbar, was stabile Lernprozesse sicherstellt. Strenge Protokolle für Logging, Governance und Latenz gewährleisten dabei reproduzierbare Entscheidungen (z.B. Shadow-Mode-Test) im Live- und Testbetrieb\[4\]\[2\].  
2\. Entscheidung & Empfehlung (Final)  
**Status:** Conditional Go. Die vorgeschlagene Kombination aus Profit- und Drawdown-Belohnung (mit späterer BSDE-Integration) ist technisch umsetzbar und basiert auf validierten Hypothesen. Entscheidendes Kriterium ist eine signifikante Performance-Verbesserung bei gleichzeitig kontrolliertem Risiko. **Go**, wenn ΔSharpe ≥ 0,10 erreicht wird und maximaler Drawdown (MaxDD) nicht über das Baseline-Niveau steigt. **No-Go** bei persistierender Instabilität oder mehrfachen Risiko-Killswitch-Auslösungen. Empfohlen wird eine stufenweise Umsetzung: zuerst das robuste N1-Reward (Profit+DD), anschließend den BSDE-Schritt.  
3\. Technische Kern-Parameter (KI-Ready)  
**RL-Framework:** PyTorch-basiert, z.B. Stable Baselines3 (SB3) als Referenz-Implementierung\[5\]\[6\]. Eigener Actor-Critic-Loop (PPO/A2C) mit custom state-space. JAX wird in Phase N1 nicht verwendet.  
**Latenz (SLA):** Entscheidung \< 50 ms, Modell-Inferenz \< 10 ms, Hard-Timeout \~80 ms. Bei Überschreitung: entweder keine Aktion (Idle) oder Position beibehalten.  
**Datenfenster:** Train 2 Jahre, Validierung 1 Jahr, Test 6 Monate; Sliding-Rolling-Splits (walk-forward), Gesamt-Anforderung ≈3,5 Jahre historische Daten. Strikte Datenintegrität (kein Leakage) durch Hash-Logging und Versionierung.  
**Datenechtheit:** Alle Datensätze werden gehasht (z.B. SHA-256) protokolliert\[6\], um Änderungen zu detektieren. Time-Based-Split verhindert Lookahead.  
**Risikobudget (Paper-Mode):** Soft-Stop bei 8% Drawdown, Hard-Stop bei 12%, Global Kill-Switch bei 15% (basierend auf maximalem Running-Drawdown über den Testzeitraum).  
4\. Reward-Funktion (Final N1 \+ Option BSDE)  
**N1-Reward (Pflicht):**  
$R\_t \= w\_p \\cdot \\tilde P\_t \\;-\\; w\_d \\cdot \\widetilde{DD}\_t \\;-\\; w\_c \\cdot I\[\\text{Risiko-Regelverletzung}\]$  
Hierbei sind $\\tilde P\_t$ normalisierter Profit, $\\widetilde{DD}\_t$ normalisierter Drawdown, und $I\[\\cdot\]$ ein Indikator (1 bei Verletzung von Risiko-Regeln wie Positions-Exposure oder Drawdown). Dieses Reward-Design spiegelt das Gleichgewicht von Rendite (höheres $\\tilde P$) und Risiko (höherer Drawdown führt zu Strafe) wider. In der Praxis hat sich gezeigt, dass solche *monotonen* Reward-Funktionen (mehr Rendite ⟹ höherer Reward, mehr Risiko ⟹ niedrigere Belohnung) mathematisch wohlgeformt sind und stabile Policy-Gradient-Updates erlauben\[3\]. In ähnlicher Weise wird hier durch $w\_p$/$w\_d$ das Trade-off justiert, analog zu Sharpe- oder Sortino-Ratio-Optimierungen in der Literatur\. \- **Option N2-Reward (mit BSDE-Risiko):**  
$R'\_t \= R\_t \\;-\\; w\_b \\cdot \\widetilde{B}\_t$  
mit $\\widetilde B\_t$ als BSDE-basierte Risiko-Schätzung (z.B. Value-at-Risk oder andere mehrdimensionale Risikomaße). Deep-BSDE-Methoden benutzen neuronale Netze, um dynamische Risikokennzahlen (z.B. zeitinkonsistente Risikomaße) in hoher Dimension approximativ zu lösen\[7\]\[8\]. Die Einbeziehung von $B\_t$ als Strafterm ermöglicht, zukünftiges Risiko vorherzusagen und entsprechend zu sanktionieren. Die Gewichte $w\_b$ regulieren den Einfluss des BSDE-Proxys auf das Reward. Bei Verletzung von Risiko-Grenzen (Exposure, Circuit-Breaker) reduziert $I\[\\cdot\]$ das Reward zusätzlich auf Null, was einen harten Killswitch-Effekt erzwingt.  
*Abb.: Schematische Architektur einer Deep-BSDE-Netzwerk-Integration für Risikomaße. Solche Netze approximieren zeitkonsistente Risikofunktionen, die als zusätzlicher Straffaktor in die Reward-Funktion eingehen können.*  
5\. State- & Action-Space (Fixiert)  
**State-Vektor:** Maximal 64 Dimensionen, gruppiert in Markt-Faktoren (\~20–30 Features), Signal-Indikatoren (\~10–15), Risiko-Kennzahlen (\~10–15). Beispielsweise zählen Liquidität, Trendindikatoren (MACD, RSI, Bollinger) sowie Realzeit-Riskositzen. Die Werte werden auf  
−1,+1  
normalisiert, um robuste Modelle sicherzustellen.  
**Action-Space (N1 – diskret):** Diskrete Positionierungsstufen {0.00 (close/no-trade), 0.25, 0.50, 0.75, 1.00, 1.25 (paper-mode extra)} für Handelsgröße. Dieses vereinfachte Mehrstufenmodell erlaubt eine stabile Exploration. (Anmerkung: In Folgeversion kann auf kontinuierlichen Action Space übergegangen werden.) Ähnlich wie in der Literatur konzipierte Agenten ein kontinuierliches Aktionsintervall \[-1,1\] für Short/Long\[9\], haben wir hier ein diskretes Raster gewählt.  
6\. Vorgehensplan (Milestones)

| Phase | Wochen | Inhalt |
| :---- | :---- | :---- |
| Prototype | 0–2 | RL-Prototyp mit Profit+Drawdown-Reward (N1), Offlinetraining, 3-Tage-Paper-Mode Test |
| Ablation | 3–5 | Varianten-Vergleich (N1 vs. nur Profit vs. Full-Reward), Hyperparameter-Optimierung, \~10–30 Paper-Blöcke |
| BSDE-PoC | 6–8 | Deep-BSDE-Modell-Implementierung, Kurzfristiger Risikoproxy, Integrationstest im Paper-Mode |
| Decision | 9 | Bewertung: Go / Conditional Go / No-Go anhand Performance-Kriterien |

7\. Messbare Tests (Reproduzierbar)  
Einheitliches Testprotokoll mit Seed-Fixierung und vollständiger Ergebnisprotokollierung. Wichtige Vorgaben: \- *Determinismus:* Zufallsgeneratoren (NumPy, PyTorch, Gym) fixieren (Set\_Seed). \- *Versionierung:* Hashes von Modell- und Datensatzversion aufzeichnen. \- *Paper-Blocks:* Mindestens 30 Evaluationsblöcke für statistische Signifikanz. \- *Ablations:* Testszenarien \= {Baseline (kein RL), nur Profit-Reward, Profit+DD, Voll-Modell}. \- *Kriterien:* ΔSharpe ≥ \+0.10 gegenüber Basisstrategie, **kein** Anstieg im maximalen Drawdown\[2\]\[3\].  
Mit dieser Methodik wird sichergestellt, dass Verbesserungen robust und nicht zufallsbedingt sind.  
8\. Machine-Readable JSON Specification (KI-bereit)  
{ "latency\_sla": { "decision\_max\_ms": 50, "model\_inference\_max\_ms": 10, "hard\_timeout\_ms": 80, "action\_on\_timeout": "no\_trade\_or\_keep\_position" }, "rl\_framework": { "core": "pytorch", "rl\_loop": "custom\_actor\_critic", "bsde\_poc": "pytorch", "reference\_framework": "stable\_baselines3", "jax\_usage": "none\_in\_n1" }, "data\_windows": { "train": "2y", "validation": "1y", "test": "6m", "min\_history": "3.5y", "split\_scheme": "rolling" }, "risk\_budget": { "soft\_dd": \-0.08, "hard\_dd": \-0.12, "killswitch\_dd": \-0.15 }, "reward": { "formula\_n1": "Rp \* w\_p \- DD \* w\_d \- constraint \* w\_c", "formula\_n2": "R\_n1 \- bsde \* w\_b", "components": \["profit", "drawdown", "risk\_constraint", "bsde\_optional"\] }, "state\_vector": { "dimensions\_max": 64, "groups": \["market", "signals", "risk"\], "scale": "normalized" }, "action\_space": { "type": "discrete", "values": \[0.0, 0.25, 0.5, 0.75, 1.0, 1.25\] }, "tests": { "paper\_blocks": 30, "ablations": \["baseline", "profit\_only", "profit\_plus\_dd", "full"\], "success": { "delta\_sharpe\_min": 0.1, "maxdd\_not\_increasing": true } }}  
**Quellen:** In aktuellen Forschungsarbeiten wird empfohlen, finanzielle RL-Belohnungen durch Kombination von Rendite- und Risikomeilensteinen (z.B. Sharpe, MaxDD) zu gestalten\[2\]\[3\]. Dynamische Risiko-Kennzahlen lassen sich mithilfe von BSDE-Modellen approximieren\[7\]\[8\]. Stable Baselines3 (SB3) bietet dabei eine breite, in PyTorch implementierte Algorithmus-Bibliothek\[5\]\[6\]. All diese Konzepte fließen in die oben skizzierte Lösung ein und bilden die Grundlage für eine robuste Umsetzung.  

